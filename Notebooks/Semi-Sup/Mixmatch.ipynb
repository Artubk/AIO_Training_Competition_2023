{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"machine_shape":"hm","gpuType":"T4","authorship_tag":"ABX9TyMNXABlEjVpCGMdZkJioZDu"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"vKueZZ6y68Ln","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1692849229014,"user_tz":-420,"elapsed":3276,"user":{"displayName":"Minh Hùng An","userId":"12219570561405750876"}},"outputId":"2841d573-ac1e-437e-8874-e19e12b32ef3"},"outputs":[{"output_type":"stream","name":"stdout","text":["Cloning into 'small_dog_cat_dataset'...\n","remote: Enumerating objects: 2608, done.\u001b[K\n","remote: Total 2608 (delta 0), reused 0 (delta 0), pack-reused 2608\u001b[K\n","Receiving objects: 100% (2608/2608), 55.84 MiB | 27.82 MiB/s, done.\n","Resolving deltas: 100% (1/1), done.\n"]}],"source":["!git clone https://github.com/anminhhung/small_dog_cat_dataset"]},{"cell_type":"code","source":["import numpy as np\n","import matplotlib.pyplot as plt\n","import torch\n","from torch.utils.data import DataLoader, Dataset, ConcatDataset, SubsetRandomSampler\n","from torchvision.datasets import MNIST\n","from torchvision import datasets, transforms\n","from torchvision.models import resnet18\n","import torchvision\n","import torch.nn as nn\n","from sklearn.model_selection import train_test_split\n","from sklearn.model_selection import KFold\n","import cv2\n","import os\n","import torch.nn.functional as F\n","\n","import albumentations"],"metadata":{"id":"XImKx6lt7IB9"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["'''\n","    Function for computing the accuracy of the predictions over the entire data_loader\n","'''\n","def get_accuracy(model, data_loader, device):\n","    correct = 0\n","    total = 0\n","\n","    with torch.no_grad():\n","        model.eval()\n","        for images, labels in data_loader:\n","            images = images.to(device)\n","            labels = labels.to(device)\n","\n","            outputs = model(images)\n","            _, predicted = torch.max(outputs.data, 1)\n","\n","            total += labels.size(0)\n","            correct += (predicted == labels).sum().item()\n","\n","    return 100*(correct/total)\n","\n","'''\n","    Function for plotting training and validation losses\n","'''\n","def plot_losses(train_acc, valid_acc, train_loss, valid_loss):\n","    # change the style of the plots to seaborn\n","    plt.style.use('seaborn')\n","\n","    train_acc = np.array(train_acc)\n","    valid_acc = np.array(valid_acc)\n","\n","    fig, (ax1, ax2) = plt.subplots(1, 2)\n","\n","    ax1.plot(train_acc, color=\"blue\", label=\"Train_acc\")\n","    ax1.plot(valid_acc, color=\"red\", label=\"Validation_acc\")\n","    ax1.set(title=\"Acc over epochs\",\n","            xlabel=\"Epoch\",\n","            ylabel=\"Acc\")\n","    ax1.legend()\n","\n","    ax2.plot(train_loss, color=\"blue\", label=\"Train_loss\")\n","    ax2.plot(valid_loss, color=\"red\", label=\"Validation_loss\")\n","    ax2.set(title=\"loss over epochs\",\n","            xlabel=\"Epoch\",\n","            ylabel=\"Loss\")\n","    ax2.legend()\n","\n","    fig.show()\n","\n","    # change the plot style to default\n","    plt.style.use('default')\n","\n","def interleave_offsets(batch, nu):\n","    groups = [batch // (nu + 1)] * (nu + 1)\n","    for x in range(batch - sum(groups)):\n","        groups[-x - 1] += 1\n","    offsets = [0]\n","    for g in groups:\n","        offsets.append(offsets[-1] + g)\n","    assert offsets[-1] == batch\n","    return offsets\n","\n","\n","def interleave(xy, batch):\n","    nu = len(xy) - 1\n","    offsets = interleave_offsets(batch, nu)\n","    xy = [[v[offsets[p]:offsets[p + 1]] for p in range(nu + 1)] for v in xy]\n","    for i in range(1, nu + 1):\n","        xy[0][i], xy[i][i] = xy[i][i], xy[0][i]\n","    return [torch.cat(v, dim=0) for v in xy]\n","\n","\n","class SemiLoss(object):\n","    def __call__(self, outputs_x, targets_x, outputs_u, targets_u):\n","        probs_u = torch.softmax(outputs_u, dim=1)\n","\n","        Lx = -torch.mean(torch.sum(F.log_softmax(outputs_x, dim=1) * targets_x, dim=1))\n","        Lu = torch.mean((probs_u - targets_u)**2)\n","\n","        return Lx, Lu\n","\n","'''\n","    function for the training step of the training loop\n","'''\n","def train(train_loader, model, criterion, optimizer, device):\n","    model.train()\n","    running_loss = 0\n","\n","    for images, labels in train_loader:\n","        images = images.to(device)\n","        labels = labels.to(device)\n","\n","        meta_images = images + torch.rand(images.size()).to(device) * 1.0 + 0.1 # augment gaussian\n","\n","        # forward pass\n","        outputs = model(images)\n","\n","        with torch.no_grad():\n","          meta_outputs = model(meta_images) # predict augment image\n","          p = torch.softmax(meta_outputs, dim=1)\n","          pt = p**(1/0.5)\n","          meta_labels = pt / pt.sum(dim=1, keepdim=True)\n","          meta_labels = torch.argmax(meta_labels, axis=1)\n","          meta_labels = meta_labels.detach()\n","\n","        l = np.random.beta(0.75, 0.75)\n","        l = max(l, 1-l)\n","\n","        input_a, input_b = images, meta_images\n","        target_a, target_b = labels, meta_labels\n","\n","        mixed_input = l * input_a + (1 - l) * input_b\n","        mixed_target = l * target_a + (1 - l) * target_b\n","        mixed_target = mixed_target.type(torch.LongTensor).to(device)\n","\n","        mix_output = model(mixed_input)\n","        loss = criterion(mix_output, mixed_target)\n","        running_loss += loss.item()\n","\n","        # backward and optimizer\n","        optimizer.zero_grad()\n","        loss.backward()\n","        optimizer.step()\n","\n","    epoch_loss = running_loss / len(train_loader)\n","\n","    return model, optimizer, epoch_loss\n","\n","'''\n","    function for the validation step of the training loop\n","'''\n","def validate(valid_loader, model, criterion, device):\n","    model.eval()\n","    running_loss = 0\n","\n","    for images, labels in valid_loader:\n","        images = images.to(device)\n","        labels = labels.to(device)\n","\n","        # forward pass and record loss\n","        outputs = model(images)\n","        loss = criterion(outputs, labels)\n","        running_loss += loss.item()\n","\n","    epoch_loss = running_loss / len(valid_loader)\n","\n","    return model, epoch_loss\n","\n","'''\n","    function defining the entire training loop\n","'''\n","def training_loop(model, criterion_ce, optimizer, train_loader, valid_loader, epochs, device, fold_idx, print_every=1):\n","    if not os.path.exists(\"save_model\"):\n","      os.mkdir(\"save_model\")\n","    # set object for storing metrics\n","    best_loss = 1e10\n","    train_losses = []\n","    valid_losses = []\n","    list_train_acc = []\n","    list_val_acc = []\n","\n","    # train model\n","    for epoch in range(0, epochs):\n","        # training\n","        model, optimizer, train_loss = train(train_loader, model, criterion_ce, optimizer, device)\n","\n","        # validation\n","        with torch.no_grad():\n","            model, valid_loss = validate(valid_loader, model, criterion_ce, device)\n","\n","        if epoch % print_every == print_every - 1:\n","            train_acc = get_accuracy(model, train_loader, device=device)\n","            valid_acc = get_accuracy(model, valid_loader, device=device)\n","\n","\n","            print('Epochs: {}, Train_loss: {}, Valid_loss: {}, Train_accuracy: {}, Valid_accuracy: {}'.format(\n","                    epoch, train_loss, valid_loss, train_acc, valid_acc\n","                    ))\n","\n","            list_train_acc.append(train_acc)\n","            list_val_acc.append(valid_acc)\n","            train_losses.append(train_loss)\n","            valid_losses.append(valid_loss)\n","\n","            # save model\n","            # if train_acc > best_acc: => moi luu\n","            torch.save(model.state_dict(), \"save_model/fold_{}_epoch_{}_acc{}.pth\".format(fold_idx+1, epoch, valid_acc))\n","            # best_acc = train_acc\n","            # print(\"[INFO] save_model/fold_{}_epoch_{}_acc{}.pth\".format(fold_idx+1, epoch, valid_acc))\n","\n","    plot_losses(list_train_acc, list_val_acc, train_losses, valid_losses)\n","\n","    return model, optimizer, (train_losses, valid_losses)"],"metadata":{"id":"ANb0eLLv7L4y"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class DogCatDataset(Dataset):\n","  def __init__(self, root_dir, transform=None):\n","    self.list_images_path = []\n","    self.list_labels = []\n","    self.one_hot_label = {\"dogs\": 0, \"cats\": 1}\n","    for sub_dir in os.listdir(root_dir):\n","      path_sub_dir = os.path.join(root_dir, sub_dir)\n","      for image_name in os.listdir(path_sub_dir):\n","        image_path = os.path.join(path_sub_dir, image_name)\n","        label = sub_dir\n","        self.list_images_path.append(image_path)\n","        self.list_labels.append(label)\n","\n","    self.transform = transform\n","\n","  def __len__(self):\n","    return len(self.list_images_path)\n","\n","  def __getitem__(self, idx):\n","    image = cv2.imread(self.list_images_path[idx])\n","    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n","    image = cv2.resize(image, (224, 224))\n","    label = np.array(self.one_hot_label[self.list_labels[idx]]) # .astype('float')\n","\n","    if self.transform:\n","      res = self.transform(image=image)\n","      image = res['image'].astype(np.float32)\n","    else:\n","      image = image.astype(np.float32)\n","\n","    image = image.transpose(2, 0, 1)\n","    sample = (image, label)\n","\n","    return sample # image, label"],"metadata":{"id":"JhkEbRBh7cHY"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def get_transforms(image_size=(224, 224)):\n","\n","    transforms_train = albumentations.Compose([\n","        albumentations.HorizontalFlip(p=0.5),\n","        albumentations.ImageCompression(quality_lower=99, quality_upper=100),\n","        albumentations.ShiftScaleRotate(shift_limit=0.2, scale_limit=0.2, rotate_limit=10, border_mode=0, p=0.7),\n","        # albumentations.Resize(image_size, image_size),\n","        # albumentations.Cutout(max_h_size=int(image_size * 0.4), max_w_size=int(image_size * 0.4), num_holes=1, p=0.5),\n","        albumentations.Normalize()\n","    ])\n","\n","    transforms_val = albumentations.Compose([\n","        albumentations.HorizontalFlip(p=0.5),\n","        albumentations.ImageCompression(quality_lower=99, quality_upper=100),\n","        albumentations.ShiftScaleRotate(shift_limit=0.2, scale_limit=0.2, rotate_limit=10, border_mode=0, p=0.7),\n","        # albumentations.Resize(image_size, image_size),\n","        # albumentations.Cutout(max_h_size=int(image_size * 0.4), max_w_size=int(image_size * 0.4), num_holes=1, p=0.5),\n","        albumentations.Normalize()\n","    ])\n","\n","    return transforms_train, transforms_val"],"metadata":{"id":"bQ1hY2og7fSo"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["transforms_train, transforms_val = get_transforms(image_size=(224, 224))"],"metadata":{"id":"TujPWRFG9Z8P"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["transformed_train_data = DogCatDataset('small_dog_cat_dataset/train', transform=transforms_train)\n","transformed_test_data = DogCatDataset('small_dog_cat_dataset/test', transform=transforms_val)"],"metadata":{"id":"JfnsHyDr8cTq"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def setup_dataflow(dataset, train_idx, val_idx):\n","    train_sampler = SubsetRandomSampler(train_idx)\n","    val_sampler = SubsetRandomSampler(val_idx)\n","\n","    train_loader = DataLoader(dataset, batch_size=32, sampler=train_sampler)\n","    val_loader = DataLoader(dataset, batch_size=32, sampler=val_sampler)\n","\n","    return train_loader, val_loader"],"metadata":{"id":"Njqm7LKJ9kt5"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class Net(nn.Module):\n","    def __init__(self):\n","        super(Net, self).__init__()\n","\n","        self.model = resnet18(num_classes=2)\n","        self.model.conv1 = nn.Conv2d(3, 64, kernel_size=3, padding=1, bias=False)\n","\n","    def forward(self, x):\n","        return self.model(x)"],"metadata":{"id":"_5USxw5g9lTu"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["num_folds = 2\n","splits = KFold(n_splits=num_folds,shuffle=True,random_state=42)"],"metadata":{"id":"rRDm5XCb9nKs"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","device"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"sYS1owcQ9r20","executionInfo":{"status":"ok","timestamp":1692849239363,"user_tz":-420,"elapsed":535,"user":{"displayName":"Minh Hùng An","userId":"12219570561405750876"}},"outputId":"30b2e17b-618e-45f7-86ca-8198e6caa8ed"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["device(type='cuda')"]},"metadata":{},"execution_count":11}]},{"cell_type":"code","source":["model = Net().to(device)"],"metadata":{"id":"ItjcCehnlpFW"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["for fold_idx, (train_idx,val_idx) in enumerate(splits.split(np.arange(len(transformed_train_data)))):\n","  print('############### Fold {} ###############'.format(fold_idx + 1))\n","  train_loader, val_loader = setup_dataflow(transformed_train_data, train_idx, val_idx)\n","  # model = Net().to(device)\n","  optimizer = torch.optim.Adam(model.parameters(), lr=0.0001) # 0.0001\n","  loss_function_ce = nn.CrossEntropyLoss()\n","  loss_function_semi = SemiLoss()\n","  model, optimizer, _ = training_loop(model, loss_function_ce, optimizer, train_loader, val_loader, 15, device, fold_idx)\n","  break\n"],"metadata":{"id":"9C8Jfh929tQi"},"execution_count":null,"outputs":[]}]}